{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://pypi.org/project/basketball-reference-scraper/\n",
    "from basketball_reference_scraper.seasons import get_schedule\n",
    "\n",
    "from basketball_reference_scraper.teams import get_roster_stats\n",
    "\n",
    "from basketball_reference_scraper.players import get_stats\n",
    "\n",
    "from basketball_reference_scraper.utils import get_player_suffix\n",
    "\n",
    "from basketball_reference_scraper.lookup import levenshtein\n",
    "\n",
    "\n",
    "import unidecode, os, sys, unicodedata\n",
    "import pandas as pd\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ['ATL', 'BRK', 'BOS', 'CHO', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU', 'IND', 'LAC', 'LAL', 'MEM', 'MIA',\n",
    "        'MIL', 'MIN', 'NOP', 'NYK', 'OKC', 'ORL', 'PHI', 'PHO', 'POR', 'SAC', 'SAS', 'TOR', 'UTA', 'WAS']\n",
    "\n",
    "years = list(range(2010,2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_stats('Jaylen Brown', 'PER_GAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev = {'ATLANTA HAWKS' : 'ATL',\n",
    "'ST. LOUIS HAWKS' : 'SLH',\n",
    "'MILWAUKEE HAWKS' : 'MIL',\n",
    "'TRI-CITIES BLACKHAWKS' : 'TCB',\n",
    "'BOSTON CELTICS' : 'BOS',\n",
    "'BROOKLYN NETS' : 'BRK',\n",
    "'NEW JERSEY NETS' : 'NJN',\n",
    "'CHICAGO BULLS' : 'CHI',\n",
    "'CHARLOTTE HORNETS (1988-2004)': 'CHH',\n",
    "'CHARLOTTE HORNETS (2014-Present)': 'CHO',\n",
    "'CHARLOTTE HORNETS': 'CHO',\n",
    "'CHARLOTTE BOBCATS' : 'CHA',\n",
    "'CLEVELAND CAVALIERS' : 'CLE',\n",
    "'DALLAS MAVERICKS': 'DAL',\n",
    "'DENVER NUGGETS' : 'DEN',\n",
    "'DETROIT PISTONS' : 'DET',\n",
    "'FORT WAYNE PISTONS' : 'FWP',\n",
    "'GOLDEN STATE WARRIORS' : 'GSW',\n",
    "'SAN FRANCISCO WARRIORS' : 'SFW',\n",
    "'PHILADELPHIA WARRIORS' : 'PHI',\n",
    "'HOUSTON ROCKETS' : 'HOU',\n",
    "'INDIANA PACERS' : 'IND',\n",
    "'LOS ANGELES CLIPPERS' : 'LAC',\n",
    "'SAN DIEGO CLIPPERS' : 'SDC',\n",
    "'BUFFALO BRAVES' : 'BUF',\n",
    "'LOS ANGELES LAKERS' : 'LAL',\n",
    "'MINNEAPOLIS LAKERS' : 'MIN',\n",
    "'MEMPHIS GRIZZLIES' : 'MEM',\n",
    "'VANCOUVER GRIZZLIES' : 'VAN',\n",
    "'MIAMI HEAT' : 'MIA',\n",
    "'MILWAUKEE BUCKS' : 'MIL',\n",
    "'MINNESOTA TIMBERWOLVES' : 'MIN',\n",
    "'NEW ORLEANS PELICANS' : 'NOP',\n",
    "'NEW ORLEANS/OKLAHOMA CITY HORNETS' : 'NOK',\n",
    "'NEW ORLEANS HORNETS' : 'NOH',\n",
    "'NEW YORK KNICKS' : 'NYK',\n",
    "'OKLAHOMA CITY THUNDER' : 'OKC',\n",
    "'SEATTLE SUPERSONICS' : 'SEA',\n",
    "'ORLANDO MAGIC' : 'ORL',\n",
    "'PHILADELPHIA 76ERS' : 'PHI',\n",
    "'SYRACUSE NATIONALS' : 'SYR',\n",
    "'PHOENIX SUNS' : 'PHO',\n",
    "'PORTLAND TRAIL BLAZERS' : 'POR',\n",
    "'SACRAMENTO KINGS' : 'SAC',\n",
    "'KANSAS CITY KINGS' : 'KCK',\n",
    "'KANSAS CITY-OMAHA KINGS' : 'KCK',\n",
    "'CINCINNATI ROYALS' : 'CIN',\n",
    "'ROCHESTER ROYALS' : 'ROR',\n",
    "'SAN ANTONIO SPURS' : 'SAS',\n",
    "'TORONTO RAPTORS' : 'TOR',\n",
    "'UTAH JAZZ' : 'UTA',\n",
    "'NEW ORLEANS JAZZ' : 'NOJ',\n",
    "'WASHINGTON WIZARDS' : 'WAS',\n",
    "'WASHINGTON BULLETS' : 'WAS',\n",
    "'CAPITAL BULLETS' : 'CAP',\n",
    "'BALTIMORE BULLETS' : 'BAL',\n",
    "'CHICAGO ZEPHYRS' : 'CHI',\n",
    "'CHICAGO PACKERS' : 'CHI',\n",
    "'ANDERSON PACKERS' : 'AND',\n",
    "'CHICAGO STAGS' : 'CHI',\n",
    "'INDIANAPOLIS OLYMPIANS' : 'IND',\n",
    "'SHEBOYGAN RED SKINS' : 'SRS',\n",
    "'ST. LOUIS BOMBERS' : 'SLB',\n",
    "'WASHINGTON CAPITOLS' : 'WAS',\n",
    "'WATERLOO HAWKS' : 'WAT'}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Make adj matrix\n",
    "id2idx = {'PHO': 0,\n",
    " 'DAL': 1,\n",
    " 'POR': 2,\n",
    " 'OKC': 3,\n",
    " 'DEN': 4,\n",
    " 'MEM': 5,\n",
    " 'WAS': 6,\n",
    " 'MIA': 7,\n",
    " 'BRK': 8,\n",
    " 'CLE': 9,\n",
    " 'TOR': 10,\n",
    " 'NOP': 11,\n",
    " 'HOU': 12,\n",
    " 'IND': 13,\n",
    " 'LAC': 14,\n",
    " 'PHI': 15,\n",
    " 'SAC': 16,\n",
    " 'UTA': 17,\n",
    " 'LAL': 18,\n",
    " 'BOS': 19,\n",
    " 'ORL': 20,\n",
    " 'MIL': 21,\n",
    " 'SAS': 22,\n",
    " 'ATL': 23,\n",
    " 'GSW': 24,\n",
    " 'CHI': 25,\n",
    " 'NYK': 26,\n",
    " 'DET': 27,\n",
    " 'MIN': 28,\n",
    " 'CHO': 29\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_stats(season):\n",
    "    # NBA season we will be analyzing\n",
    "    year = season\n",
    "    # URL page we will scraping (see image above)\n",
    "    url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\".format(year)\n",
    "    # this is the HTML from the given URL\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    #organize it into a list:\n",
    "    # use findALL() to get the column headers\n",
    "    soup.findAll('tr', limit=2)\n",
    "    # use getText()to extract the text we need into a list\n",
    "    headers = [th.getText() for th in soup.findAll('tr', limit=2)[0].findAll('th')]\n",
    "    # exclude the first column as we will not need the ranking order from Basketball Reference for the analysis\n",
    "    headers = headers[1:]\n",
    "\n",
    "\n",
    "    # avoid the first header row\n",
    "    rows = soup.findAll('tr')[1:]\n",
    "    player_stats = [[td.getText() for td in rows[i].findAll('td')]\n",
    "                for i in range(len(rows))]\n",
    "\n",
    "    stats = pd.DataFrame(player_stats, columns = headers)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_ranks(season):\n",
    "    rankings = pd.read_csv('./data/rank{}.csv'.format(season))\n",
    "    rankings['Team'] = rankings['Team'].apply(lambda x: abbrev[x.upper()])\n",
    "    rankings = rankings.set_index('Team').drop(columns=['Overall'])\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_season_stats(2010)\n",
    "rankings = get_season_ranks(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/feat2011.csv')\n",
    "\n",
    "\n",
    "features = features.set_index(features['Tm'].map(id2idx)).drop(columns=['Tm'])\n",
    "features = features.sort_index()\n",
    "\n",
    "labels = torch.Tensor(features['Rk'].to_numpy())\n",
    "features = torch.Tensor(features.drop(columns='Rk').to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc11 = pd.read_csv('data/sch2011')\n",
    "\n",
    "adj = np.zeros((30, 30), dtype='float32')\n",
    "\n",
    "rows = sc11['Home']\n",
    "cols = sc11['Away']\n",
    "\n",
    "for i in np.arange(len(sc11)):\n",
    "    adj[ id2idx[rows[i]], id2idx[cols[i]]] = adj[ id2idx[rows[i]], id2idx[cols[i]]] + 1.0\n",
    "    \n",
    "adj = torch.Tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0001 loss_train: 40750.8203 acc_train: 0.0333 loss_val: 40750.8203 acc_val: 0.0333 time: 0.0040s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0002 loss_train: 27182.6738 acc_train: 0.0333 loss_val: 27182.6738 acc_val: 0.0333 time: 0.0037s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0003 loss_train: 19494.5703 acc_train: 0.0333 loss_val: 19494.5703 acc_val: 0.0333 time: 0.0037s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0004 loss_train: 11794.2393 acc_train: 0.0333 loss_val: 11794.2393 acc_val: 0.0333 time: 0.0036s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0005 loss_train: 10551.7881 acc_train: 0.0333 loss_val: 10551.7881 acc_val: 0.0333 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0006 loss_train: 8618.1309 acc_train: 0.0333 loss_val: 8618.1309 acc_val: 0.0333 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0007 loss_train: 5322.6631 acc_train: 0.0000 loss_val: 5322.6631 acc_val: 0.0000 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0008 loss_train: 3968.3687 acc_train: 0.0333 loss_val: 3968.3687 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0009 loss_train: 3485.4509 acc_train: 0.0333 loss_val: 3485.4509 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0010 loss_train: 2789.4165 acc_train: 0.0333 loss_val: 2789.4165 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0011 loss_train: 1605.5331 acc_train: 0.0333 loss_val: 1605.5331 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0012 loss_train: 3.4354 acc_train: 0.0333 loss_val: 3.4354 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0013 loss_train: 3.4352 acc_train: 0.0333 loss_val: 3.4352 acc_val: 0.0333 time: 0.0030s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0014 loss_train: 3.4350 acc_train: 0.0333 loss_val: 3.4350 acc_val: 0.0333 time: 0.0030s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0015 loss_train: 3.4347 acc_train: 0.0333 loss_val: 3.4347 acc_val: 0.0333 time: 0.0030s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0016 loss_train: 15.1743 acc_train: 0.0000 loss_val: 15.1743 acc_val: 0.0000 time: 0.0030s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0017 loss_train: 3.4343 acc_train: 0.0333 loss_val: 3.4343 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0018 loss_train: 3.4341 acc_train: 0.0333 loss_val: 3.4341 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0019 loss_train: 3.4340 acc_train: 0.0333 loss_val: 3.4340 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0020 loss_train: 3.4338 acc_train: 0.0333 loss_val: 3.4338 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0021 loss_train: 3.4337 acc_train: 0.0333 loss_val: 3.4337 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0022 loss_train: 5.8281 acc_train: 0.0333 loss_val: 5.8281 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0023 loss_train: 8.7257 acc_train: 0.0333 loss_val: 8.7257 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0024 loss_train: 8.5861 acc_train: 0.0333 loss_val: 8.5861 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0025 loss_train: 3.4332 acc_train: 0.0333 loss_val: 3.4332 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0026 loss_train: 3.4330 acc_train: 0.0333 loss_val: 3.4330 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0027 loss_train: 3.4329 acc_train: 0.0333 loss_val: 3.4329 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0028 loss_train: 3.4328 acc_train: 0.0333 loss_val: 3.4328 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0029 loss_train: 3.4326 acc_train: 0.0333 loss_val: 3.4326 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0030 loss_train: 3.4325 acc_train: 0.0333 loss_val: 3.4325 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0031 loss_train: 15.2278 acc_train: 0.0333 loss_val: 15.2278 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0032 loss_train: 4.4229 acc_train: 0.0333 loss_val: 4.4229 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0033 loss_train: 3.4322 acc_train: 0.0333 loss_val: 3.4322 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0034 loss_train: 3.4321 acc_train: 0.0333 loss_val: 3.4321 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0035 loss_train: 3.4644 acc_train: 0.0333 loss_val: 3.4644 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0036 loss_train: 3.4319 acc_train: 0.0333 loss_val: 3.4319 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0037 loss_train: 3.4317 acc_train: 0.0333 loss_val: 3.4317 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0038 loss_train: 3.4316 acc_train: 0.0333 loss_val: 3.4316 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0039 loss_train: 3.4315 acc_train: 0.0333 loss_val: 3.4315 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0040 loss_train: 3.4314 acc_train: 0.0333 loss_val: 3.4314 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0041 loss_train: 3.4313 acc_train: 0.0333 loss_val: 3.4313 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0042 loss_train: 3.4312 acc_train: 0.0333 loss_val: 3.4312 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0043 loss_train: 3.4311 acc_train: 0.0333 loss_val: 3.4311 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0044 loss_train: 3.4310 acc_train: 0.0333 loss_val: 3.4310 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0045 loss_train: 3.4309 acc_train: 0.0333 loss_val: 3.4309 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0046 loss_train: 3.4307 acc_train: 0.0333 loss_val: 3.4307 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0047 loss_train: 3.4306 acc_train: 0.0333 loss_val: 3.4306 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0048 loss_train: 3.4305 acc_train: 0.0333 loss_val: 3.4305 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0049 loss_train: 3.4304 acc_train: 0.0333 loss_val: 3.4304 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0050 loss_train: 3.4303 acc_train: 0.0333 loss_val: 3.4303 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0051 loss_train: 3.4301 acc_train: 0.0333 loss_val: 3.4301 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0052 loss_train: 3.4300 acc_train: 0.0333 loss_val: 3.4300 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0053 loss_train: 3.4299 acc_train: 0.0333 loss_val: 3.4299 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0054 loss_train: 3.4297 acc_train: 0.0333 loss_val: 3.4297 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0055 loss_train: 3.4296 acc_train: 0.0333 loss_val: 3.4296 acc_val: 0.0333 time: 0.0039s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0056 loss_train: 3.4295 acc_train: 0.0333 loss_val: 3.4295 acc_val: 0.0333 time: 0.0035s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0057 loss_train: 3.4294 acc_train: 0.0333 loss_val: 3.4294 acc_val: 0.0333 time: 0.0037s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0058 loss_train: 3.4292 acc_train: 0.0333 loss_val: 3.4292 acc_val: 0.0333 time: 0.0035s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0059 loss_train: 3.4291 acc_train: 0.0333 loss_val: 3.4291 acc_val: 0.0333 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0060 loss_train: 3.4290 acc_train: 0.0333 loss_val: 3.4290 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0061 loss_train: 3.4288 acc_train: 0.0333 loss_val: 3.4288 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0062 loss_train: 3.4287 acc_train: 0.0333 loss_val: 3.4287 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0063 loss_train: 3.4286 acc_train: 0.0333 loss_val: 3.4286 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0064 loss_train: 3.4285 acc_train: 0.0333 loss_val: 3.4285 acc_val: 0.0333 time: 0.0035s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0065 loss_train: 3.4283 acc_train: 0.0333 loss_val: 3.4283 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0066 loss_train: 3.4282 acc_train: 0.0333 loss_val: 3.4282 acc_val: 0.0333 time: 0.0029s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0067 loss_train: 3.4281 acc_train: 0.0333 loss_val: 3.4281 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0068 loss_train: 3.4279 acc_train: 0.0333 loss_val: 3.4279 acc_val: 0.0333 time: 0.0041s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0069 loss_train: 3.4278 acc_train: 0.0333 loss_val: 3.4278 acc_val: 0.0333 time: 0.0037s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0070 loss_train: 3.4277 acc_train: 0.0333 loss_val: 3.4277 acc_val: 0.0333 time: 0.0037s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0071 loss_train: 3.4276 acc_train: 0.0333 loss_val: 3.4276 acc_val: 0.0333 time: 0.0035s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0072 loss_train: 3.4274 acc_train: 0.0333 loss_val: 3.4274 acc_val: 0.0333 time: 0.0035s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0073 loss_train: 3.4273 acc_train: 0.0333 loss_val: 3.4273 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0074 loss_train: 3.4272 acc_train: 0.0333 loss_val: 3.4272 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0075 loss_train: 3.4271 acc_train: 0.0333 loss_val: 3.4271 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0076 loss_train: 3.4269 acc_train: 0.0333 loss_val: 3.4269 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0077 loss_train: 3.4268 acc_train: 0.0333 loss_val: 3.4268 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0078 loss_train: 3.4267 acc_train: 0.0333 loss_val: 3.4267 acc_val: 0.0333 time: 0.0033s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0079 loss_train: 3.4266 acc_train: 0.0333 loss_val: 3.4266 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0080 loss_train: 3.4265 acc_train: 0.0333 loss_val: 3.4265 acc_val: 0.0333 time: 0.0030s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0081 loss_train: 3.4263 acc_train: 0.0333 loss_val: 3.4263 acc_val: 0.0333 time: 0.0031s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0082 loss_train: 3.4262 acc_train: 0.0333 loss_val: 3.4262 acc_val: 0.0333 time: 0.0025s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0083 loss_train: 3.4261 acc_train: 0.0333 loss_val: 3.4261 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0084 loss_train: 3.4260 acc_train: 0.0333 loss_val: 3.4260 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0085 loss_train: 3.4259 acc_train: 0.0333 loss_val: 3.4259 acc_val: 0.0333 time: 0.0028s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0086 loss_train: 3.4258 acc_train: 0.0333 loss_val: 3.4258 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0087 loss_train: 3.4257 acc_train: 0.0333 loss_val: 3.4257 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0088 loss_train: 3.4255 acc_train: 0.0333 loss_val: 3.4255 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0089 loss_train: 3.4254 acc_train: 0.0333 loss_val: 3.4254 acc_val: 0.0333 time: 0.0027s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0090 loss_train: 3.4253 acc_train: 0.0333 loss_val: 3.4253 acc_val: 0.0333 time: 0.0026s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0091 loss_train: 3.4252 acc_train: 0.0333 loss_val: 3.4252 acc_val: 0.0333 time: 0.0025s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0092 loss_train: 3.4251 acc_train: 0.0333 loss_val: 3.4251 acc_val: 0.0333 time: 0.0025s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0093 loss_train: 3.4250 acc_train: 0.0333 loss_val: 3.4250 acc_val: 0.0333 time: 0.0026s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0094 loss_train: 3.4249 acc_train: 0.0333 loss_val: 3.4249 acc_val: 0.0333 time: 0.0025s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0095 loss_train: 3.4248 acc_train: 0.0333 loss_val: 3.4248 acc_val: 0.0333 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0096 loss_train: 3.4247 acc_train: 0.0333 loss_val: 3.4247 acc_val: 0.0333 time: 0.0034s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0097 loss_train: 3.4246 acc_train: 0.0333 loss_val: 3.4246 acc_val: 0.0333 time: 0.0033s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0098 loss_train: 3.4245 acc_train: 0.0333 loss_val: 3.4245 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0099 loss_train: 3.4244 acc_train: 0.0333 loss_val: 3.4244 acc_val: 0.0333 time: 0.0032s\n",
      "torch.Size([30, 31]) torch.Size([30])\n",
      "Epoch: 0100 loss_train: 3.4242 acc_train: 0.0333 loss_val: 3.4242 acc_val: 0.0333 time: 0.0032s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3238s\n",
      "Test set results: loss= 3.4241 accuracy= 0.0333\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import argparse\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Training settings\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    \n",
    "    print(output.shape, labels.shape)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_train = loss(output, labels.type(torch.LongTensor))\n",
    "    acc_train = accuracy(output, labels)\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    loss_val = loss(output, labels.type(torch.LongTensor))\n",
    "    acc_val = accuracy(output, labels)\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss_test = loss(output, labels.type(torch.LongTensor))\n",
    "    acc_test = accuracy(output, labels)\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "\n",
    "    \n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16,\n",
    "            nclass=len(labels) + 1,\n",
    "            dropout=.5)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=.01, weight_decay=5e-4)\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-61353e8b2d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sparse_softmax import Sparsemax\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import spspmm, coalesce\n",
    "\n",
    "#from layers import GCN, HGPSLPool\n",
    "\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        value = edge_index.new_ones((edge_index.size(1),), dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n)\n",
    "        value.fill_(0)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n)\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_features = args.num_features\n",
    "        self.nhid = args.nhid\n",
    "        self.num_classes = args.num_classes\n",
    "        self.pooling_ratio = args.pooling_ratio\n",
    "        self.dropout_ratio = args.dropout_ratio\n",
    "        self.sample = args.sample_neighbor\n",
    "        self.sparse = args.sparse_attention\n",
    "        self.sl = args.structure_learning\n",
    "        self.lamb = args.lamb\n",
    "\n",
    "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
    "        self.conv2 = GCN(self.nhid, self.nhid)\n",
    "        self.conv3 = GCN(self.nhid, self.nhid)\n",
    "\n",
    "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
    "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
    "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        edge_attr = None\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
